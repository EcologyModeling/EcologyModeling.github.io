---
title: "Выбор и оценка статистических методов моделирования"
output:
  html_document:
    highlight: textmate
    toc_float: false
    includes:
      before_body: [include_header.html, include_lessons_nav.html]
      after_body: [include_lessons_nav_end.html, include_footer.html]
---
<div class="alert alert-danger" role="alert"> Раздел находится в разработке !</div>

## Пакет dplyr

Моделирование в той или иной форме является составной частью более сложного процесса, через который проходит любой исследователь работающий с реальными данными -  анализ данных. Не вдаваясь глубоко в подробности в данном разделе будет рассмотрен начальный этап анализа данных - преобразования или манипуляции с данными (data-manipulation).

В целом мы уже рассматривали некоторые функции языка R, которые можно использовать для манипцляции данными, меж тем помимо классической парадигмы существует ряд широко распространненных пакетов, которые позволяют проводить ряд операций в более явном виде и используя меньше кода. Одним из самых популярных подобных пакетов является пакет dplyr из целой "вселенной" пакетов tydiverse от известного разработчика Хадли Викхема.

```{r}
library("tidyverse")
library("nycflights13")
```
Изучение пакета dplyr мы начнем со следующих функций:

  * filter()
  * arrange()
  * select()
  * mutate()
  * summarize()
  * group_by()
  
  
filter() allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. For example, we can select all flights on January 1st with:

```{r}

filter(flights, month == 1, day == 1)
```

The following code finds all flights that departed in November or December:

```{r}
filter(flights, month == 11 | month == 12)
```

A useful shorthand for this problem is x %in% y. This will select every row where x is one of the values in y. We could use it to rewrite the preceding code:

```{r}
nov_dec <- filter(flights, month %in% c(11, 12))
```

arrange() works similarly to filter() except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:

```{r}
arrange(flights, year, month, day)
arrange(flights, desc(arr_delay))
```

Функция desc() говорит о том, что колонку нужно пересортировать от большего к меньшему

It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables.

```{r}
select(flights, year, month, day)
```

Интересной особенностью является возможностью выбирать ряд колонок меж двух заданных

```{r}
select(flights, year:day)
select(flights, -(year:day))
```

Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. That’s the job of mutate().
mutate() always adds new columns at the end of your dataset so we’ll start by creating a narrower dataset so we can see the new variables. Remember that when you’re in RStudio, the easiest way to see all the columns is View():

```{r}
flights_sml <- select(flights, year:day, ends_with("delay"), distance, air_time )
mutate(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60)
```

The last key verb is summarize(). It collapses a data frame to a single row:

```{r}
summarize(flights, delay = mean(dep_delay, na.rm = TRUE))
```

summarize() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group.” For example, if we applied exactly the same code to a data frame grouped by date, we get the average delay per date:

```{r}
by_day <- group_by(flights, year, month, day)
summarize(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

Together group_by() and summarize() provide one of the tools that you’ll use most commonly when working with dplyr: grouped summaries. But before we go any further with this, we need to introduce a powerful new idea: the pipe.

Imagine that we want to explore the relationship between the distance and average delay for each location. Using what you know about dplyr, you might write code like this:



###Классификация данных

+------------------------------+------------------------------------------+
|*Machine learning terminology*|*Typical Algorithm*                       |
+------------------------------+------------------------------------------+
|Classification: assigning     | Decision trees                           |
|known labels to objects       | Naive Bayes                              |
|                              | Logistic regression (witha threshold)    |
|                              | Support vector machines                  |
+------------------------------+------------------------------------------+
|Regression: predicting or     | Linear regression                        |
|forecasting numerical         | Logistic regression                      |
|values                        |                                          |
+------------------------------+------------------------------------------+
|Association rules:            | Apriori                                  | 
|finding objects that          |                                          |
|tend to appear in             |                                          |
|the data together             |                                          |
+------------------------------+------------------------------------------+
|Clustering: finding groups of | K-means                                  |
|objects that are more similar |                                          |
|to each other than to objects |                                          |
|in other groups               |                                          |
+------------------------------+------------------------------------------+
|Nearest neighbor: predicting  | Nearest neighbor                         |
|property of a datum based on  |                                          |  
|the datum or data that are    |                                          |
|most similar to it            |                                          |
+------------------------------+------------------------------------------+


When building a model, the first thing to check is if the model even works on the data
it was trained from. In this section, we do this by introducing quantitative measures of
model performance. From an evaluation point of view, we group model types this way.

1. Classification
2. Scoring
3. Probability estimation
4. Raking
5. Clustering
 
+------------+------------------------------------------------------------------------------------+
|Null model  |A null model is the best model of a very simple form you’re trying to outperform.   |
|            |The two most typical null model choices are a model that is a single                |
|            |constant (returns the same answer for all situations) or a model that is            |
|            |independent (doesn’t record any important relation or interaction between           |
|            |inputs and outputs). We use null models to lower-bound desired performance,         |
|            |so we usually compare to a best null model. For example, in a categorical           |
|            |problem, the null model would always return the most popular                        |
|            |category (as this is the easy guess that is least often wrong); for a score         |
|            |model, the null model is often the average of all the outcomes (as this has         |
|            |the least square deviation from all of the outcomes); and so on. The idea is        |
|            |this: if you’re not out-performing the null model, you’re not delivering value.     |
|            |Note that it can be hard to do as good as the best null model, because even         |
|            |though the null model is simple, it’s privileged to know the overall distribution   |
|            |of the items it will be quizzed on. We always assume the null model                 |
|            |we’re comparing to is the best of all possible null models.                         |
+------------+------------------------------------------------------------------------------------+
|Bayes rate  |A Bayes rate model (also sometimes called a saturated model) is a best              |
|model       |possible model given the data at hand. The Bayes rate model is the perfect          |
|            |model and it only makes mistakes when there are multiple examples with              |
|            |the exact same set of known facts (same xs) but different outcomes (different       |
|            |ys). It isn’t always practical to construct the Bayes rate model, but we            |
|            |invoke it as an upper bound on a model evaluation score.                            |
|            |If we feel our model is performing significantly above the null model rate and      |
|            |is approaching the Bayes rate, then we can stop tuning. When we have a lot          |
|            |of data and very few modeling features, we can estimate the Bayes error             |
|            |rate. Another way to estimate the Bayes rate is to ask several different people     |
|            |to score the same small sample of your data; the found inconsistency                |
|            |rate can be an estimate of the Bayes rate.a                                         |
+------------+------------------------------------------------------------------------------------+
 
 
 

+---------------------------+-------------------------------+---------------------------------------+ 
|                           |Prediction=NEGATIVE            |Prediction=POSITIVE                    |
+---------------------------+-------------------------------+---------------------------------------+ 
|Truth mark=NOT IN CATEGORY |True negatives (TN) cM[1,1]=264|False positives (FP) cM[1,2]=14        |
+---------------------------+-------------------------------+---------------------------------------+ 
|Truth mark=IN CATEGORY     |False negatives (FN) cM[2,1]=22| True positives (TP) cM[2,2]=158       |
+---------------------------+-------------------------------+---------------------------------------+



+-------------+---------------------+
|Measure      | Formula             |
+-------------+---------------------+
|Accuracy     |(TP+TN)/(TP+FP+TN+FN)|
+-------------+---------------------+
|Precision    |TP/(TP+FP)           |
+-------------+---------------------+
|Recall       |TP/(TP+FN)           |
+-------------+---------------------+
|Sensitivity  |TP/(TP+FN)           |
+-------------+---------------------+
|Specificity  |TN/(TN+FP)           |
+-------------+---------------------+

###Make a table
+------------+------------------------------------------------------------------------------------+
|Measure     |Typical business need Follow-up question Accuracy “We need most of our decisions to |
|            |be correct.”“Can we tolerate being wrong 5% of the time? And do users see mistakes  |
|            | like spam marked as non-spam or non-spam marked as spam as being equivalent?”      |
+------------+------------------------------------------------------------------------------------+
|Precision   |“Most of what we marked as spam had darn well better be spam.” That would guarantee |
|            |that most of what is in the spam folder is in fact spam but it isn’t the best way to| 
|            |measure what fraction of the user’s legitimate email is lost. We could cheat on this| 
|            |goal by sending all our users a bunch of easy-to-identify spam that we correctly    |
|            |identify. Maybe we really want good specificity.”                                   |
+------------+------------------------------------------------------------------------------------+
|Recall      | “We want to cut down on the amount of spam a user sees by a factor of 10 (eliminate|
|            | 90% of the spam).” “If 10% of the spam gets through, will the user see mostly      |
|            | non-spam mail or mostly spam? Will this result in a good user experience?”         |
+------------+------------------------------------------------------------------------------------+
|Sensitivity |“We have to cut a lot of spam, otherwise  the user won’t see a benefit.”            |
|            |“If we cut spam down to 1% of what it is now,would that be a good user experience?” |
|            |                                                                                    |
+------------+------------------------------------------------------------------------------------+
|Specificity |“We must be at least three nines on legitimate email; the user must see at least    |         
|            | 99.9%of their non-spam email.” “Will the user tolerate missing 0.1% of their       |
|            |legitimate email, and should we keep a spam folder the user can look at?”           |
+------------+------------------------------------------------------------------------------------+
 

CORRELATION
Correlation is very helpful in checking if variables are potentially useful in a model. Be
advised that there are at least three calculations that go by the name of correlation:
Pearson, Spearman, and Kendall (see help(cor)). The Pearson coefficient checks for
linear relations, the Spearman coefficient checks for rank or ordered relations, and
the Kendall coefficient checks for degree of voting agreement. Each of these coefficients
performs a progressively more drastic transform than the one before and has
well-known direct significance tests (see help(cor.test)).

***DON’T USE CORRELATION TO EVALUATE MODEL QUALITY IN PRODUCTION***

It’s tempting to use correlation to measure model quality, but we advise against it.
The problem is this: correlation ignores shifts and scaling factors. So correlation
is actually computing if there is any shift and rescaling of your predictor
that is a good predictor. This isn’t a problem for training data (as these predictions
tend to not have a systematic bias in shift or scaling by design) but
can mask systematic errors that may arise when a model is used in production.


***THE WORST POSSIBLE MODELING OUTCOME***
The worst possible modeling outcome is not failing to find a good model. The worst possible modeling outcome
is thinking you have a good model when you don’t. One of the easiest
ways to accidentally build such a deficient model is to have an instrumental or
independent variable that is in fact a subtle function of the outcome. Such
variables can easily leak into your training data, especially when you have no
knowledge or control of variable meaning preparation. The point is this: such
variables won’t actually be available in a real deployment and often are in
training data packaged up by others.
2